{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ineventhorizon/AI-Course-Progress-Example-Code/blob/main/AINewsSummarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers sumy rouge-score nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cWO58fsS9y7",
        "outputId": "c99eae6c-9eea-426d-9878-80d8e3caf2ab",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score, breadability, docopt\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5077e172d73b0b3ee7128403e2e9fb10fc542a2bf0219442c39a27f83d5a29d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=631e1d6ebc1ed830935f73bba33cd39ba30130f8d76e47d6d042c25a9e867ad0\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f2023a1e0a04b4cf1d8748e3667646b4756b0bf126ed026f0de45081e491d4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built rouge-score breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy, rouge-score\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 rouge-score-0.1.2 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rmisra/news-category-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "miNxdyd2j0qT",
        "outputId": "e832ee90-646c-420b-ffd1-49e9a30479c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rmisra/news-category-dataset?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.5M/26.5M [00:00<00:00, 61.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rmisra/news-category-dataset/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')  # Often used with other nltk functionalities\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqZUjisoTyNU",
        "outputId": "0c98d0b7-4775-4df8-da92-486aafa21d49",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K5W6yMkSwtT",
        "outputId": "af0b437a-79e1-4ab8-c8cb-da28158128f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Summary:\n",
            " Extractive Summary:\n",
            "Artificial intelligence (AI) is at the forefront of innovation, revolutionizing sectors such as healthcare, technology, and transportation. Advanced AI systems, like generative models such as GPT-4, are transforming the way industries operate. These models enable applications such as virtual assistants, content creation, and sophisticated data analysis, providing unprecedented efficiency and scalability. In healthcare, AI is improving patient outcomes by powering diagnostics, predictive analytics, and personalized treatment plans. Similarly, in technology, AI is accelerating software development and enabling intelligent automation.\n",
            "\n",
            "Abstractive Summary:\n",
            "Artificial intelligence (AI) is at the forefront of innovation, revolutionizing sectors such as healthcare, technology, and transportation. However, alongside these advancements, ethical concerns loom large. Issues such as algorithmic bias, data privacy, and misuse of AI technologies present significant challenges.\n",
            "\n",
            "Personalized Summary:\n",
            " Artificial intelligence (AI) is at the forefront of innovation, revolutionizing sectors such as healthcare, technology, and transportation. Issues such as algorithmic bias, data privacy, and misuse of AI technologies present significant challenges.\n",
            "\n",
            "Evaluation Scores (ROUGE):\n",
            " {'rouge1': Score(precision=0.65625, recall=0.38181818181818183, fmeasure=0.48275862068965525), 'rougeL': Score(precision=0.53125, recall=0.3090909090909091, fmeasure=0.3908045977011494)}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# User profile for personalization\n",
        "user_profile = {\n",
        "    \"topics\": [\"technology\", \"AI\"],\n",
        "    \"sentiment\": \"neutral\",\n",
        "    \"summary_length\": 5  # Number of sentences\n",
        "}\n",
        "\n",
        "# Step 1: Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    # Basic cleaning of text\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    return text\n",
        "\n",
        "# Step 2: Extractive summarization using LexRank\n",
        "def extractive_summary(text, num_sentences=5):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return \" \".join([str(sentence) for sentence in summary])\n",
        "\n",
        "# Step 3: Abstractive summarization using transformers\n",
        "def abstractive_summary(text):\n",
        "    summarization_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarization_pipeline(text, max_length=130, min_length=30, do_sample=False)\n",
        "    return summary[0][\"summary_text\"]\n",
        "\n",
        "# Step 4: Personalization of the summary\n",
        "def personalize_summary(summary, user_profile):\n",
        "    # Filter sentences based on user preferences\n",
        "    personalized_sentences = []\n",
        "    for sentence in nltk.sent_tokenize(summary):\n",
        "        if any(topic.lower() in sentence.lower() for topic in user_profile[\"topics\"]):\n",
        "            personalized_sentences.append(sentence)\n",
        "    return \" \".join(personalized_sentences[: user_profile[\"summary_length\"]])\n",
        "\n",
        "# Step 5: Evaluation using ROUGE\n",
        "def evaluate_summary(reference_summary, generated_summary):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    return scores\n",
        "\n",
        "def main():\n",
        "    article = \"\"\"\n",
        "    Artificial intelligence (AI) is at the forefront of innovation, revolutionizing sectors such as healthcare, technology, and transportation. Advanced AI systems, like generative models such as GPT-4, are transforming the way industries operate. These models enable applications such as virtual assistants, content creation, and sophisticated data analysis, providing unprecedented efficiency and scalability.\n",
        "\n",
        "In healthcare, AI is improving patient outcomes by powering diagnostics, predictive analytics, and personalized treatment plans. Similarly, in technology, AI is accelerating software development and enabling intelligent automation. The transportation sector is witnessing the emergence of self-driving cars and optimized logistics systems driven by AI.\n",
        "\n",
        "However, alongside these advancements, ethical concerns loom large. Issues such as algorithmic bias, data privacy, and misuse of AI technologies present significant challenges. For instance, biased algorithms can reinforce existing societal inequities, while the misuse of generative models raises concerns about misinformation and security risks. Addressing these challenges requires robust frameworks for transparency, fairness, and accountability in AI development and deployment.\n",
        "\n",
        "As AI continues to evolve, it remains a double-edged sword: a tool for transformative progress and a source of profound ethical dilemmas. Striking the right balance will be key to ensuring that AI serves as a force for good in society.\n",
        "    \"\"\"\n",
        "    # Reference summary for evaluation\n",
        "    reference_summary = \"\"\"\n",
        "    AI is revolutionizing industries such as healthcare, technology, and transportation with advancements like GPT-4, which enable applications such as diagnostics, automation, and virtual assistants. While these developments promise significant benefits, they are accompanied by ethical challenges, including algorithmic bias and misuse concerns. Balancing innovation with accountability is essential for the responsible evolution of AI.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process the article\n",
        "    clean_article = preprocess_text(article)\n",
        "\n",
        "    # Generate summaries\n",
        "    extractive = extractive_summary(clean_article, num_sentences=user_profile[\"summary_length\"])\n",
        "    abstractive = abstractive_summary(clean_article)\n",
        "\n",
        "    # Combine both summaries for a hybrid approach\n",
        "    hybrid_summary = f\"Extractive Summary:\\n{extractive}\\n\\nAbstractive Summary:\\n{abstractive}\"\n",
        "\n",
        "    # Personalize the summary\n",
        "    personalized_summary = personalize_summary(abstractive, user_profile)\n",
        "\n",
        "    # Evaluate the summary\n",
        "    scores = evaluate_summary(reference_summary, personalized_summary)\n",
        "\n",
        "    # Output results\n",
        "    print(\"Hybrid Summary:\\n\", hybrid_summary)\n",
        "    print(\"\\nPersonalized Summary:\\n\", personalized_summary)\n",
        "    print(\"\\nEvaluation Scores (ROUGE):\\n\", scores)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPUDvCjcTg2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWYpe7CGSzfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}